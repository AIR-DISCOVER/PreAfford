<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8" />
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description"
    content="PreAfford: Universal Affordance-Based Pre-Grasping for Diverse Objects and Environments. Robotic manipulation of ungraspable objects with two-finger grippers presents significant challenges due to the paucity of graspable features, while traditional pre-grasping techniques, which rely on repositioning objects and leveraging external aids like table edges, lack the adaptability across object categories and scenes. Addressing this, we introduce PreAfford, a novel pre-grasping planning framework that utilizes a point-level affordance representation and a relay training approach to enhance adaptability across a broad range of environments and object types, including those previously unseen. Demonstrated on the ShapeNet-v2 dataset, PreAfford significantly improves grasping success rates by 69% and validates its practicality through real-world experiments. This work offers a robust and adaptable solution for manipulating ungraspable objects." />

  <meta name="keywords"
    content="PreAfford: Universal Affordance-Based Pre-Grasping for Diverse Objects and Environments" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>
    PreAfford | Project Page
  </title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico" />
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />

  <link rel="stylesheet" href="static/css/bulma.min.css" />
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css" />
  <link rel="stylesheet" href="static/css/bulma-slider.min.css" />
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
  <link rel="stylesheet" href="static/css/index.css" />
  <link rel="stylesheet" href="https://unpkg.com/beerslider/dist/BeerSlider.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h2 class="title is-2 publication-title">
              PreAfford: Universal Affordance-Based Pre-Grasping for Diverse Objects and Environments
            </h2>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->

              <span class="author-block">
                Kairui Ding
                <sup>1</sup>
              ,</span>

              <span class="author-block">
                Boyuan Chen
                <sup>1</sup>
              ,</span>

              <span class="author-block">
                <a href="https://warshallrho.github.io/" target="_blank">Ruihai Wu</a>
                <sup>2</sup>
              ,</span>

              <span class="author-block">
                <a href="https://yuyangli.com/" target="_blank">Yuyang Li</a>
                <sup>3</sup>
              ,</span>

              <span class="author-block">
                Zongzheng Zhang
                <sup>1</sup>
              ,</span>

              <span class="author-block">
                <a href="https://c7w.tech/about/" target="_blank">Huan-ang Gao</a>
                <sup>1</sup>
              ,</span>

              <span class="author-block">
                Siqi Li
                <sup>1</sup>
              ,</span>

              <span class="author-block">
                <a href="https://yzhu.io/" target="_blank">Yixin Zhu</a>
                <sup>3</sup>
              ,</span>

              <span class="author-block">
                Guyue Zhou
                <sup>1,4</sup>
              ,</span>

              <span class="author-block">
                <a href="https://zsdonghao.github.io/" target="_blank">Hao Dong</a>
                <sup>2</sup>
              ,</span>

              <span class="author-block">
                <a href="https://sites.google.com/view/fromandto" target="_blank">Hao Zhao</a>
                <sup>&dagger;1</sup>
              </span>
            </div>
            <!-- a margin of 0.5em -->
            <div style="margin: 0.5em;"></div>
            <div class="is-size-5 publication-authors">
              <span class="author-block is-size-6">
                <sup>1</sup> Institute for AI Industry Research (AIR), Tsinghua University
                <br>
                <sup>2</sup> CFCS, School of Computer Science, Peking University &nbsp;&nbsp;&nbsp;
                <sup>3</sup> Institute for Artificial Intelligence, Peking University  <br/>
                <sup>4</sup> School of Vehicle and Mobility, Tsinghua University <br/>
                <!-- <div style="margin: 0.1em;"></div> -->
                <!-- <span class="eql-cntrb"><small><br /><sup>*</sup>Indicates Equal Contribution</small></span>
                <!-- a span of 5em -->
                <!-- <span style="margin: 1em;"></span>  -->
                <span class="eql-cntrb"><small><sup>&dagger;</sup>Indicates Corresponding Author</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/xxxx.yyyyy.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Github link -->
                <!-- <span class="link-block">
                  <a href="https://github.com/AIR-DISCOVER/SCP-Diff-Toolkit" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/xxxx.yyyyy" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Teaser video-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/intro_website_ffmpeg.mp4" type="video/mp4" />
        </video>
        <!-- centering the image -->
        <!-- <div class="columns is-centered">
          <!-- <div class="column is-four-fifths"> -->
          <!-- <div class="publication-video"> -->
          <!-- <img src="static/images/Teaser_cs1.jpg" width="100%" /> -->
          <!-- </div> -->
          <!-- </div> -->
        <!-- </div> -->
        <!-- <img src="static/images/Teaser_cs1.jpg" width="100%" /> -->
        <h2 class="has-text-centered is-size-6">
         Demonstration Video of <b>PreAfford</b>.
        </h2>
      </div>
    </div>
  </section>
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
Robotic manipulation of ungraspable objects with two-finger grippers presents significant challenges due to the paucity of graspable features, while traditional pre-grasping techniques, which rely on repositioning objects and leveraging external aids like table edges, lack the adaptability across object categories and scenes. Addressing this, we introduce <b>PreAfford</b>, a novel pre-grasping planning framework that utilizes a point-level affordance representation and a relay training approach to enhance adaptability across a broad range of environments and object types, including those previously unseen. Demonstrated on the ShapeNet-v2 dataset, <b>PreAfford</b> significantly improves grasping success rates by 69% and validates its practicality through real-world experiments. This work offers a robust and adaptable solution for manipulating ungraspable objects.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <!-- Youtube video -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        
        <h2 class="title is-3">Introduction & Method</h2>

        <div class="columns is-centered has-text-centered"
          style="width: 100%; display: flex; justify-content: center; align-items: center; flex-direction: column">
          <div style="width: 70%; display: flex; justify-content: center; align-items: stretch; flex-direction: row">
            <div style="width: 100%;">
              <img src="static/images/teaser.png" alt="">
            </div>
          </div>
          <div style="width: 70%;">
            Introduction to <b>PreAfford</b>. Adopting a <b>relay training</b> paradigm, two successive modules collaborate to handle the grasping tasks of ungraspable objects utilizing different environmental features (edge, slope, slot and wall).
          </div>
        </div>
        <br /><br />

        <div class="columns is-centered has-text-centered"
          style="width: 100%; display: flex; justify-content: center; align-items: center; flex-direction: column">
          <div style="width: 70%; display: flex; justify-content: center; align-items: stretch; flex-direction: row">
            <div style="width: 100%;">
              <img src="static/images/framework.png" alt="">
            </div>
          </div>
          <div style="width: 70%;">
            <b>Framework of PreAfford.</b> Pre-grasping and grasping modules are included, each containing an affordance network, proposal network, and critic network. In the inference phase, the two modules process point clouds to devise strategies for pre-grasping and grasping. Conversely, during training, the grasping module generates rewards to train the pre-grasping module, which we intuitively call <b>relay</b>.
          </div>
        </div>

      </div>
    </div>
  </section>
  <!-- End youtube video -->

  <!-- Youtube video -->
  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <!-- Paper video. -->
        <h2 class="title is-3">Results</h2>
        
        <div class="columns is-centered has-text-centered"
          style="width: 100%; display: flex; justify-content: center; align-items: center; flex-direction: column">

          <div style="width: 70%; display: flex; justify-content: center; align-items: stretch; flex-direction: row">
            <div style="width: 100%;">
              <img src="static/images/main.png" alt="">
            </div>
          </div>

          <div style="width: 70%;"><b>Qualitative results.</b> Both training and testing categories across four distinct scenarios (edge, slot, slope, and wall) are tested. Affordance maps illustrate potential effective interaction areas. It can be observed that <b>PreAfford</b> yields reasonable pre-grasping and grasping strategies across different object categories and scenes on both seen and unseen objects.</div>

          <div style="margin: 1em;"></div>

          <div style="width: 50%; display: flex; justify-content: center; align-items: stretch; flex-direction: row">
            <div style="width: 100%;">
              <img src="static/images/multiple.png" alt="">
            </div>
          </div>

          <div style="width: 70%;"><b>Multi-feature environment adaptability.</b> (a) Rendered image of the multi-feature environment. (b) Point cloud and affordance hot map, showcasing reasonable pre-grasping policies for objects at different locations.</div>

          <div style="margin: 1em;"></div>

          <div style="width: 85%;"><b>Table: Comparison with baselines.</b> Pre-grasping significantly improves grasping success rates by 52.9%. Closed-loop strategy further improves success rates by 16.4% over all categories. A push in a random direction and a push <b>towards</b> the geometric center are also included as baselines.</div>

          <!-- Table with training and testing object categories -->
          <table border="1">
            <tr>
              <th rowspan="2"><strong>Setting</strong></th>
              <th colspan="6"><strong>Train object categories</strong></th>
              <th colspan="6"><strong>Test object categories</strong></th>
            </tr>
            <tr>
              <td><strong>Edge</strong></td>
              <td><strong>Wall</strong></td>
              <td><strong>Slope</strong></td>
              <td><strong>Slot</strong></td>
              <td><strong>Multi</strong></td>
              <td><strong>Avg.</strong></td>
              <td><strong>Edge</strong></td>
              <td><strong>Wall</strong></td>
              <td><strong>Slope</strong></td>
              <td><strong>Slot</strong></td>
              <td><strong>Multi</strong></td>
              <td><strong>Avg.</strong></td>
            </tr>
            <tr>
              <td>W/o pre-grasping</td>
              <td>2.3</td>
              <td>3.8</td>
              <td>4.3</td>
              <td>3.4</td>
              <td>4.0</td>
              <td>3.6</td>
              <td>6.1</td>
              <td>2.3</td>
              <td>2.9</td>
              <td>5.7</td>
              <td>6.0</td>
              <td>4.6</td>
            </tr>
            <tr>
              <td>Random-direction Push</td>
              <td>21.6</td>
              <td>10.3</td>
              <td>6.4</td>
              <td>16.8</td>
              <td>18.1</td>
              <td>14.6</td>
              <td>24.9</td>
              <td>17.2</td>
              <td>12.1</td>
              <td>18.4</td>
              <td>23.0</td>
              <td>19.1</td>
            </tr>
            <tr>
              <td>Center-point Push</td>
              <td>32.5</td>
              <td>23.7</td>
              <td>40.5</td>
              <td>39.2</td>
              <td>39.0</td>
              <td>35.0</td>
              <td>25.1</td>
              <td>17.4</td>
              <td>28.0</td>
              <td>30.2</td>
              <td>21.5</td>
              <td>24.4</td>
            </tr>
            <tr>
              <td>Ours w/o closed-loop</td>
              <td>67.2</td>
              <td>41.5</td>
              <td>58.3</td>
              <td>76.9</td>
              <td>63.6</td>
              <td>61.5</td>
              <td>56.4</td>
              <td>37.3</td>
              <td>62.6</td>
              <td>75.8</td>
              <td>55.4</td>
              <td>57.5</td>
            </tr>
            <tr>
              <td><strong>Ours</strong></td>
              <td><strong>81.4</strong></td>
              <td><strong>43.4</strong></td>
              <td><strong>73.1</strong></td>
              <td><strong>83.5</strong></td>
              <td><strong>74.1</strong></td>
              <td><strong>71.1</strong></td>
              <td><strong>83.7</strong></td>
              <td><strong>47.6</strong></td>
              <td><strong>80.5</strong></td>
              <td><strong>83.0</strong></td>
              <td><strong>74.6</strong></td>
              <td><strong>73.9</strong></td>
            </tr>
          </table>

          <div style="margin: 1em;"></div>
          
          <div style="width: 70%; display: flex; justify-content: center; align-items: stretch; flex-direction: row">
            <div style="width: 100%;">
              <img src="static/images/experiment.png" alt="">
            </div>
          </div>
            
          <div style="width: 70%;"><b>Real-world experiment figures.</b> Four pre-grasping cases with their affordance map are demonstrated: (a) Move a tablet to table edge, (b) Push a plate towards wall, (c) Push a keyboard up onto a slope and (d) Slide a tablet into a slot.</div>

          <div style="margin: 1em;"></div>
          
          <div style="width: 85%;"><b>Table: Real-world experiment results.</b> For both direct grasping (without pre-grasping) and grasping after pre-grasping cases, we perform two experiments on each object in each experimental scene, success rates are shown below as percentages.</div>
          
            <!-- Table with seen and unseen categories -->
            <table border="1">
              <tr>
                <th rowspan="2"><strong>Setting</strong></th>
                <th colspan="6"><strong>Seen categories</strong></th>
                <th colspan="6"><strong>Unseen categories</strong></th>
              </tr>
              <tr>
                <td><strong>Edge</strong></td>
                <td><strong>Wall</strong></td>
                <td><strong>Slope</strong></td>
                <td><strong>Slot</strong></td>
                <td><strong>Multi</strong></td>
                <td><strong>Avg.</strong></td>
                <td><strong>Edge</strong></td>
                <td><strong>Wall</strong></td>
                <td><strong>Slope</strong></td>
                <td><strong>Slot</strong></td>
                <td><strong>Multi</strong></td>
                <td><strong>Avg.</strong></td>
              </tr>
              <tr>
                <td>W/o pre-grasping</td>
                <td>0</td>
                <td>0</td>
                <td>0</td>
                <td>0</td>
                <td>0</td>
                <td>0</td>
                <td>10</td>
                <td>0</td>
                <td>5</td>
                <td>0</td>
                <td>0</td>
                <td>3</td>
              </tr>
              <tr>
                <td>With pre-grasping</td>
                <td>70</td>
                <td>45</td>
                <td>80</td>
                <td>90</td>
                <td>85</td>
                <td>74</td>
                <td>80</td>
                <td>30</td>
                <td>75</td>
                <td>90</td>
                <td>85</td>
                <td>72</td>
              </tr>
            </table>

        </div>
      </div>
    </div>
  </section>
  <!-- End youtube video -->

  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      If you find our work useful in your research, please consider citing:
      <div style="margin: 0.5em;"></div>
      <pre><code>todo</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the
              <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project
                Page Template</a>
              which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the of this website, we
              just ask that you link back to this page in the footer. <br />
              This website is licensed under a
              <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons
                Attribution-ShareAlike 4.0 International
                License</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->
  <script src="https://unpkg.com/beerslider/dist/BeerSlider.js"></script>
  <script>
    new BeerSlider(document.getElementById('slider1'), { start: '40' });
    new BeerSlider(document.getElementById('slider2'), { start: '40' });
    new BeerSlider(document.getElementById('slider3'), { start: '40' });
    new BeerSlider(document.getElementById('slider4'), { start: '40' });
    new BeerSlider(document.getElementById('slider5'), { start: '40' });
    new BeerSlider(document.getElementById('slider6'), { start: '40' });
    new BeerSlider(document.getElementById('slider7'), { start: '40' });
    new BeerSlider(document.getElementById('slider8'), { start: '40' });
    new BeerSlider(document.getElementById('slider9'), { start: '40' });
    new BeerSlider(document.getElementById('slider10'), { start: '40' });
  </script>
</body>

</html>
